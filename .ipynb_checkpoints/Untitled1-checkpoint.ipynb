{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RewardPredictorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Predict the reward that a human would assign to each frame of\n",
    "    the input trajectory, trained using the human's preferences between\n",
    "    pairs of trajectories.\n",
    "    Network inputs:\n",
    "    - s1/s2     Trajectory pairs\n",
    "    - pref      Preferences between each pair of trajectories\n",
    "    Network outputs:\n",
    "    - r1/r2     Reward predicted for each frame\n",
    "    - rs1/rs2   Reward summed over all frames for each trajectory\n",
    "    - pred      Predicted preference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(RewardPredictorNetwork, self).__init__()\n",
    "        nh, nw, nc = env.observation_space.shape\n",
    "        nact = env.action_space.n\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(nc, 8, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(nw))))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(nh))))\n",
    "        \n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.head1 = nn.Linear(linear_input_size, 16)\n",
    "        self.head2 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, s1s, s2s):        \n",
    "        assert s1s.shape == s2s.shape, \"segments should be the same shape\"\n",
    "        \n",
    "        shape = s1s.shape\n",
    "        batch_size = shape[0]\n",
    "        segment_length = shape[1]\n",
    "        \n",
    "        #wrapping segment_length and batch_size, so size[0] is 'batchsize * segment_length'\n",
    "        s1s = torch.reshape(s1s, ([-1] + list(s1s.shape[2:])))\n",
    "        s2s = torch.reshape(s2s, ([-1] + list(s2s.shape[2:])))\n",
    "        \n",
    "        #stack segments and batchsize together, so new shape is '2 * batchsize * segment_length'\n",
    "        x = torch.cat((s1s, s2s), axis = 0)\n",
    "        \n",
    "        #pass segment data through convolutional model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.head1(x))\n",
    "        x = self.head2(x)\n",
    "    \n",
    "        #Split rewards at each frame back into individual segments\n",
    "        #where shape is 'batchsize * segment_length'\n",
    "        r1s, r2s = torch.split(x, batch_size * segment_length)\n",
    "        \n",
    "        #unwrapping segment_length and batch_size\n",
    "        r1s =  torch.reshape(r1s, [batch_size, segment_length, -1])\n",
    "        r2s =  torch.reshape(r2s, [batch_size, segment_length, -1])\n",
    "        \n",
    "        #Sum over all the segment_length to get a shape of 'batch_size'\n",
    "        r1 = torch.sum(r1s, axis = 1)\n",
    "        r2 = torch.sum(r2s, axis = 1)\n",
    "        \n",
    "        #Predict human preference for each segment\n",
    "        rs = torch.cat((r1, r2), axis = 1)\n",
    "        pred = self.softmax(rs)\n",
    "        \n",
    "        return r1s, r2s, pred\n",
    "    \n",
    "    def predict_single(self, s1s):\n",
    "        \n",
    "        #pass segment data through convolutional model\n",
    "        x = F.relu(self.bn1(self.conv1(s1s)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.head1(x))\n",
    "        r1s = self.head2(x)\n",
    "        \n",
    "        return r1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictorEnsemble(nn.Module):\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 lr=3e-4,\n",
    "                 n_preds=1):\n",
    "        \n",
    "        super(RewardPredictorEnsemble, self).__init__()\n",
    "        self.n_preds = n_preds\n",
    "        self.rps = []\n",
    "        for pred_n in range(n_preds):\n",
    "            rp = RewardPredictorNetwork(env)\n",
    "            self.rps.append(rp)\n",
    "    \n",
    "        self.n_steps = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return concatenation of all the reward networks outputs\n",
    "        \"\"\"\n",
    "        return torch.cat([rp(x.clone()) for rp in self.rps], dim=1)\n",
    "    \n",
    "    def raw_reward(self, x):\n",
    "        \"\"\"\n",
    "        Returns reward at each frame for a batch of clips\n",
    "        \"\"\"\n",
    "        stack = torch.stack([rp.predict_single(x.clone()) for rp in self.rps], dim=0)\n",
    "        return torch.squeeze(stack)\n",
    "    \n",
    "    def reward(self, x):\n",
    "        \"\"\"\n",
    "        Return (normalized) reward for each frame of a single segment.\n",
    "        (Normalization involves normalizing the rewards from each member of the\n",
    "        ensemble separately, then averaging the resulting rewards across all\n",
    "        ensemble members.)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            ensemble_rewards = self.raw_reward(x).permute(1,0)\n",
    "            \n",
    "        print(ensemble_rewards.shape)\n",
    "            \n",
    "        ensemble_rewards -= ensemble_rewards.mean(axis = 0)\n",
    "        ensemble_rewards /= (ensemble_rewards.std(axis = 0)+1e-12)\n",
    "        ensemble_rewards *= 0.05\n",
    "        \n",
    "        rs = torch.mean(ensemble_rewards, axis=0)\n",
    "        \n",
    "        return rs\n",
    "    \n",
    "    def preferences(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Predict probability of human preferring one segment over another\n",
    "        for each segment in the supplied batch of segment pairs.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, prefs_train, prefs_val, val_interval):\n",
    "        \"\"\"\n",
    "        Train all ensemble members for one epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Training with {} prefrences and {} Test\".format(len(prefs_train), len(prefs_val)))\n",
    "        \n",
    "        start_steps = self.n_steps\n",
    "        start_time = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "r = RewardPredictorEnsemble(env=env, n_preds = 10)\n",
    "\n",
    "pred = r.reward(torch.rand((30, 3, 210, 160)))\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-cf90de4079b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.transpose(pred, 0, 1).std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f674b2afd50>,\n",
       " <matplotlib.lines.Line2D at 0x7f674b2aff50>,\n",
       " <matplotlib.lines.Line2D at 0x7f66b4d280d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f66b4d28310>,\n",
       " <matplotlib.lines.Line2D at 0x7f66b4d284d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f66b4d28790>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([r(torch.zeros((1, 3, 210, 160))).std(axis=0).detach().numpy() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictorNetwork():\n",
    "    \"\"\"\n",
    "    Predict the reward any given human would assign to the pair of \n",
    "    input trajectories, trained using human prefrences\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 3, 210, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
